

####################################################
# [1] ë„¤ì´ë²„ ë‰´ìŠ¤ api í™œìš©í•´ì„œ ëŒ€ìƒ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
####################################################

def load_news(query='ë³´í—˜ +AI', n=100):
  print('ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.')

  # ë„¤ì´ë²„ API ì¸ì¦ ì •ë³´
  client_key = client_id + ':' + client_secret
  client_key_enc = b64encode(client_key.encode('utf-8')).decode('utf-8')

  # API í˜¸ì¶œì„ ìœ„í•œ URL
  url = 'https://openapi.naver.com/v1/search/news.json'
  headers = {'X-Naver-Client-Id': client_id, 'X-Naver-Client-Secret': client_secret}

  # ê²€ìƒ‰ ìš”ì²­ íŒŒë¼ë¯¸í„°
  params = {
      'query': query,
      'display': 100,  # ê²€ìƒ‰ ê²°ê³¼ ì¤‘ 100ê°œ ê°€ì ¸ì˜´(í•œ ë²ˆì— ìµœëŒ€ 100ê°œ, ì´ˆê³¼ ì‹œ ì˜¤ë¥˜ ë°˜í™˜)
      'sort': 'date',  # ë‚ ì§œìˆœ ì •ë ¬(date) ì •í™•ë„ ìˆœ ì •ë ¬(sim)
      'start': 1
  }

  news_list = []
  for start_idx in range(1, n+1, 10):  # 1ë¶€í„° 100ê¹Œì§€ 10ì”© ì¦ê°€
      params['start'] = start_idx
      response = requests.get(url, headers=headers, params=params)

      # API ì‘ë‹µ í™•ì¸
      if response.status_code != 200:
          print(f"Failed to get news data at start index {start_idx}: {response.status_code}")
          continue

      news_data = response.json()

      # 'items' í‚¤ê°€ ìˆëŠ”ì§€ í™•ì¸
      if 'items' not in news_data:
          print("No 'items' key in the API response.")
          continue

      # ê° ë‰´ìŠ¤ì˜ URLê³¼ ì œëª©ì„ ê°€ì ¸ì™€ì„œ ë°˜í™˜
      for item in news_data['items']:
          news_list.append({
              'title': item['title'],
              'pubDate' : item['pubDate'],
              'url': item['link'],
              'originallink' : item['originallink']
          })
  df = pd.DataFrame(news_list)
  print("--- '{}' ê²€ìƒ‰ ê²°ê³¼, ë‰´ìŠ¤ {}ê±´ì„ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.".format(query, len(news_list)))  # 1000

  ## ë„¤ì´ë²„ë‰´ìŠ¤ ê±°ë¥´ê¸°
  df = df[df['url'].str.contains('https://n.news.naver.com')].reset_index(drop=True)
  print("--- ì „ì²´ ë‰´ìŠ¤ {}ê±´ ì¤‘ ë„¤ì´ë²„ ë‰´ìŠ¤ëŠ” {}ê±´ì…ë‹ˆë‹¤.".format(len(news_list), len(df)))

  # df.to_excel('ë„¤ì´ë²„ë‰´ìŠ¤ê¸°ì‚¬í¬ë¡¤ë§_{}.xlsx'.format(datetime.now(pytz.utc).astimezone(pytz.timezone('Asia/Seoul')).strftime('%m%d_%H:%M')), index=False)
  return df



## ì–¸ë¡ ì‚¬ oid ì¶”ì¶œ ë° matching

def get_oid(df):
  ## ë„¤ì´ë²„ ì–¸ë¡ ì‚¬ ë‰´ìŠ¤ í™ˆì—ì„œ oid í¬ë¡¤ë§
  html_company = urllib.request.urlopen('https://news.naver.com/main/officeList.naver').read() #ì–¸ë¡ ì‚¬ ë‰´ìŠ¤ í™ˆ ì½ê¸°
  soup_company = BeautifulSoup(html_company, 'html.parser')
  title_company = soup_company.find_all(class_='list_press nclicks(\'rig.renws2pname\')')

  cmp_name = []
  cmp_oid = []
  for i in title_company:
    parts = urlparse(i.attrs['href'])
    cmp_name.append(i.get_text().strip())
    cmp_oid.append(parse_qs(parts.query)['officeId'][0])
  oid = pd.DataFrame(list(zip(cmp_name, cmp_oid)), columns=['news_name', 'oid'])
  oid.drop_duplicates(inplace=True) # ì¤‘ë³µì œê±°

  ## í¬ë¡¤ë§ëª»í•œ ì–¸ë¡ ì‚¬ ì¶”ê°€
  add_oid = pd.DataFrame({'news_name':['ë§ˆì´ë°ì¼ë¦¬', 'ìŠ¤í¬ì¸ ê²½í–¥', 'ìŠ¤í¬ì¸ ì„œìš¸', 'ì¼ê°„ìŠ¤í¬ì¸ '],
                          'oid':['117', '144', '468', '241']})
  oid = pd.concat([oid, add_oid], ignore_index=True)

  ## ì–¸ë¡ ì‚¬ ìˆœìœ„ ì…ë ¥ (2024.05.30 ê¸°ì¤€)
  # ìˆœìœ„ ì°¸ê³  : https://gobooki.net/%EC%96%B8%EB%A1%A0%EC%82%AC-%EB%84%A4%EC%9D%B4%EB%B2%84-%EC%B1%84%EB%84%90-%EC%B5%9C%EC%8B%A0-%EA%B5%AC%EB%8F%85%EC%9E%90-%EC%88%98-%EC%A0%95%EB%A6%AC/
  news_name_list = [
      "JTBC", "YTN", "MBC", "SBS", "êµ­ë¯¼ì¼ë³´", "ë§¤ì¼ê²½ì œ", "ì¡°ì„ ì¼ë³´", "ì¤‘ì•™ì¼ë³´", "í•œêµ­ê²½ì œ", "KBS",
      "í•œê²¨ë ˆ", "ê²½í–¥ì‹ ë¬¸", "ë™ì•„ì¼ë³´", "ë¨¸ë‹ˆíˆ¬ë°ì´", "ì„œìš¸ê²½ì œ", "ì„œìš¸ì‹ ë¬¸", "ì•„ì‹œì•„ê²½ì œ", "ì—°í•©ë‰´ìŠ¤", "ì´ë°ì¼ë¦¬", "ì¡°ì„ ë¹„ì¦ˆ",
      "íŒŒì´ë‚¸ì…œë‰´ìŠ¤", "í•œêµ­ì¼ë³´", "í—¤ëŸ´ë“œê²½ì œ", "MBN", "ë‰´ìŠ¤1", "ë‰´ì‹œìŠ¤", "ë””ì§€í„¸íƒ€ì„ìŠ¤", "ë¬¸í™”ì¼ë³´", "ë¶€ì‚°ì¼ë³´", "ì—°í•©ë‰´ìŠ¤TV",
      "í•œêµ­ê²½ì œTV", "ë…¸ì»·ë‰´ìŠ¤", "SBSBiz", "SBS Biz", "ë¯¸ë””ì–´ì˜¤ëŠ˜", "kbcê´‘ì£¼ë°©ì†¡", "TVì¡°ì„ ", "ê°•ì›ì¼ë³´", "ë°ì¼ë¦¬ì•ˆ", "ë§¤ì¼ì‹ ë¬¸",
      "ë¹„ì¦ˆë‹ˆìŠ¤ì›Œì¹˜", "ë¹„ì¦ˆì›Œì¹˜", "ì„¸ê³„ì¼ë³´", "ì‹œì‚¬ì €ë„", 'ë§ˆì´ë°ì¼ë¦¬', "ì•„ì´ë‰´ìŠ¤24", "ì˜¤ë§ˆì´ë‰´ìŠ¤", "ì „ìì‹ ë¬¸", "ì±„ë„A", "í”„ë ˆì‹œì•ˆ",
      "êµ­ì œì‹ ë¬¸", "ë†ë¯¼ì‹ ë¬¸", "ë¸”ë¡œí„°", "ê²½ê¸°ì¼ë³´", "ë‰´ìŠ¤íƒ€íŒŒ",

      "ì „ì£¼MBC", "ì¡°ì„¸ì¼ë³´", "ê°•ì›ë„ë¯¼ì¼ë³´", "ëŒ€ì „ì¼ë³´", "ë”íŒ©íŠ¸",
      "ë””ì§€í„¸ë°ì¼ë¦¬", "ë§¤ê²½ì´ì½”ë…¸ë¯¸", "ì‹œì‚¬IN", "ì´ì½”ë…¸ë¯¸ìŠ¤íŠ¸", "ì£¼ê°„ì¡°ì„ ", "ì§€ë””ë„·ì½”ë¦¬ì•„", "ì½”ë¦¬ì•„ì¤‘ì•™ë°ì¼ë¦¬", "ì½”ë¦¬ì•„í—¤ëŸ´ë“œ", "ì½”ë©”ë””ë‹·ì»´", "í•œê²¨ë ˆ21",
      "í•œê²½ë¹„ì¦ˆë‹ˆìŠ¤", "í—¬ìŠ¤ì¡°ì„ ",

      'ìŠ¤í¬ì¸ ì„œìš¸', 'ì¼ê°„ìŠ¤í¬ì¸ ', 'ì—¬ì„±ì‹ ë¬¸', 'ë¨¸ë‹ˆS', 'ë ˆì´ë””ê²½í–¥', 'ìŠ¤í¬ì¸ ê²½í–¥', 'ì‹ ë™ì•„', 'ì£¼ê°„ë™ì•„', 'ë™ì•„ì‚¬ì´ì–¸ìŠ¤',
      'ëŒ€êµ¬MBC', 'ì¼ë‹¤', 'CJBì²­ì£¼ë°©ì†¡', 'ê¸°ìí˜‘íšŒë³´', 'ì›”ê°„ ì‚°', 'JIBS', 'ë”ìŠ¤ì¿ í”„', 'ì£¼ê°„ê²½í–¥', 'ì¤‘ì•™SUNDAY'
  ]

  news_rank = pd.DataFrame({'news_name': news_name_list, 'news_rank': list(range(1, int(len(news_name_list)) + 1))})
  oid = pd.merge(oid, news_rank, how='left', on='news_name')

  ## ì–¸ë¡ ì‚¬ëª… ë§¤ì¹­
  df['oid'] = df.url.str.split('/').str[5] # í…Œì´ë¸”ì— ì–¸ë¡ ì‚¬ oid ì¶”ê°€
  df = pd.merge(df, oid, how='left', on='oid')
  df.reset_index(drop=True, inplace=True)

  # ì–¸ë¡ ì‚¬ëª… ë§¤ì¹­ ì•ˆ ëì„ ê²½ìš° ì˜ˆì™¸ì²˜ë¦¬
  df.news_name.fillna('-', inplace=True)
  df.news_rank.fillna(99, inplace=True)

  if len(df[df.news_name=='-'])>0:
    print('--- rank ë§¤ì¹­ ì•ˆ ëœ ì–¸ë¡ ì‚¬ëª… : \n', df[df.news_name=='-'][['oid', 'originallink']])
  else:
    print('--- rank ë§¤ì¹­ ì•ˆ ëœ ì–¸ë¡ ì‚¬ëª… : ì—†ìŒ')

  return df



####################################################
## [2] ì „ì²˜ë¦¬
####################################################

# data cleasing (html ì œê±°)
def clean_html(x):
  x = re.sub("\&\w*\;","", x)  # & ; í˜•íƒœì˜ html ê°’ ì œê±°
  x = re.sub("<.*?>","", x) # <> í˜•íƒœì˜ htmlê°’ ì œê±°
  x = re.sub(r'\s+', ' ', x) # ê³µë°± ì—¬ëŸ¬ê°œì¸ ê²½ìš° 1ê°œë¡œ ì¤„ì—¬ì¤Œ
  x = re.sub(r'â€˜|â€™|â€œ|â€|`|â€²', "'", x) # â€˜â€™â€œâ€ -> 'ë¡œ í†µì¼
  x = re.sub(r'&quot;|\.\.\.|â€¦', '', x)
  return x

def news_preprocessing(df):
  print('\në‰´ìŠ¤ ì „ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.')

  # ì œëª© data cleasing (html ì œê±°)
  df['title'] = df['title'].apply(lambda x: clean_html(x))
  print('--- Data Cleansing ì™„ë£Œ')

  # ê°€ì¥ ë†’ì€ ë­í‚¹ì˜ ë‰´ìŠ¤ë¥¼ ë‚¨ê¸°ê³  ì œëª© ì¤‘ë³µì œê±°
  df = df.sort_values(by='news_rank')
  df.drop_duplicates(subset=['title'], keep='first', inplace=True)
  df.reset_index(drop=True, inplace=True)
  print("--- ê°€ì¥ ë†’ì€ ë­í‚¹ì˜ ë‰´ìŠ¤ë¥¼ ë‚¨ê¸°ê³  ì œëª© ì¤‘ë³µì œê±° í›„", len(df),"ê±´")

  # ì–¸ë¡ ì‚¬ ìˆœìœ„ê¶Œ ì´ë‚´ ê¸°ì‚¬ë§Œ ì¶œë ¥
  # ranking=55
  # df = df[df.news_rank <=ranking]
  # print("ì–¸ë¡ ì‚¬ ìˆœìœ„ "+str(ranking)+"ìœ„ê¶Œ ì´ë‚´ ê¸°ì‚¬ í•„í„°ë§ í›„ ë‰´ìŠ¤ ê°œìˆ˜", len(df))

  # íŠ¹ì •ë¬¸ì í¬í•¨ ì œëª© ì œê±°
  del_keywords='ë‚˜ìŠ¤ë‹¥|ì½”ìŠ¤í”¼|ìŠ¹ì§„|ì„ëª…|í—¤ë“œë¼ì¸|ì¦ì‹œ|\(ì¢…í•©\)|í‘œì°½|ê¸°ê³ |ê³µì‹œ|ì£¼ê°€ì‹œí™©|ë¦¬í¬íŠ¸|.*\[.*\].*|ìƒì¥|ç¾|ä¸­|ë¶€ê³ '
  df = df[df.title.str.contains(del_keywords)==False].reset_index(drop=True)
  print("--- íŠ¹ì •ë¬¸ì í¬í•¨ ì œëª© ì œê±° í›„ ë‰´ìŠ¤ ê°œìˆ˜", len(df),"ê±´")

  return df



## ë‰´ìŠ¤ ì¤‘ë³µì œê±°
def del_dup_news(df, col):
  # TF-IDF ë²¡í„°í™” ë° ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
  vectorizer = TfidfVectorizer()
  tfidf_matrix = vectorizer.fit_transform(df[col])
  cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)

  # ìœ ì‚¬ë„ì— ê¸°ë°˜í•œ êµ°ì§‘í™”
  clustering_model = AgglomerativeClustering(n_clusters=None, distance_threshold=0.5, affinity='cosine', linkage='average')
  clusters = clustering_model.fit_predict(cosine_sim)

  df['cluster'] = clusters
  print('--- TF-IDFê¸°ë°˜ êµ°ì§‘í™” ì™„ë£Œ')

  # ê° í´ëŸ¬ìŠ¤í„°ì—ì„œ news_rankê°€ ê°€ì¥ ì‘ì€ í–‰ë§Œ ë‚¨ê¸°ê¸°
  df_dep_dup = df.loc[df.groupby('cluster')['news_rank'].idxmin()].reset_index(drop=True)
  print('--- ê°™ì€ cluster ë‚´ ì–¸ë¡ ì‚¬ ìˆœìœ„ê°€ ê°€ì¥ ë†’ì€ ë‰´ìŠ¤ë§Œ ë‚¨ê¸°ê³  ì¤‘ë³µ ì œê±° í›„', len(df_dep_dup), 'ê±´')

  return df_dep_dup


####################################################
# [3] ì£¼ìš” ë‰´ìŠ¤ë§Œ ë½‘ê¸°
####################################################

def gpt_imp_news(df):
  global imp_news_yn
  global imp_news_prompt

  print('\nì¤‘ìš”í•œ ë‰´ìŠ¤ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.')

  imp_news_prompt = '''
ë„ˆëŠ” [ê¸°ì‚¬ ì œëª©]ì„ ë³´ê³  ë³´í—˜ì‚¬ì˜ ë””ì§€í„¸í˜ì‹ ê´€ë ¨ ê¸°ì‚¬ì¸ì§€ ë¶„ë¥˜ë¥¼ ì˜í•˜ëŠ” ë¶„ë¥˜ê°€ì•¼.

ì•„ë˜ì— ìˆëŠ” [ê¸°ì‚¬ ì œëª©]ì„ ë³´ê³  "ê¸ˆìœµê¶Œ"ì˜
DX, Digital, ë””ì§€í„¸í˜ì‹ , AI, ìƒì„±í˜•AI, í´ë¼ìš°ë“œ, ë§ˆì´ë°ì´í„°, í—¬ìŠ¤ì¼€ì–´, ìš”ì–‘ì‚°ì—…, ì‹¤ë²„ì‚°ì—…, AICC, ìë™í™”, ì–´í”Œë¦¬ì¼€ì´ì…˜
ì™€ ê´€ë ¨ëœ ë‰´ìŠ¤ê°€ ë§ìœ¼ë©´ 1, ì•„ë‹ˆë©´ 0ìœ¼ë¡œ ì¶œë ¥í•´ì¤˜.

ë°˜ë“œì‹œ 1ê³¼ 0ë§Œ ì¶œë ¥í•´ì•¼í•´
------------------------------------------
[ê¸°ì‚¬ ì œëª©] :
'''

  client = OpenAI()
  start_time = time.time()
  imp_news_yn = []
  for i in tqdm(range(len(df))):
    completion = client.chat.completions.create(
      model="gpt-3.5-turbo", # gpt-3.5-turbo
      messages=[
        {"role": "system", "content": "ê¸°ì‚¬ ì œëª©ì„ ë³´ê³  ê¸ˆìœµê¶Œì˜ ë””ì§€í„¸í˜ì‹ ê´€ë ¨ ê¸°ì‚¬ì¸ì§€ ë¶„ë¥˜ë¥¼ ì˜í•˜ëŠ” ë¶„ì„ê°€"},
        {"role": "user", "content": imp_news_prompt + df['title'][i]}
      ],
      temperature=0, # ìµœëŒ€í•œ ë‹µë³€ì„ ë‹¤ë¥´ê²Œ í•˜ì§€ ì•Šë„ë¡ í•¨
      max_tokens=1 # 0, 1ë§Œ ì¶œë ¥í•˜ë„ë¡ max tokenì„ ë‚®ê²Œ ì§€ì •
    )
    imp_news_yn.append(completion.choices[0].message.content)
  df['imp_news_yn'] = imp_news_yn
  end_time = time.time()
  print("--- ì¤‘ìš”ë‰´ìŠ¤ê¸°ì‚¬ ì„ ë³„ì™„ë£Œ --- ì†Œìš”ëœ ì‹œê°„(ì´ˆ):", end_time - start_time)


  ## ì¤‘ìš”ë‰´ìŠ¤ê¸°ì‚¬ë¶„ë¥˜ ì €ì¥
  no_imp = df[df['imp_news_yn']=='0'].title.sort_values().tolist()
  imp = df[df['imp_news_yn']=='1'].title.sort_values().tolist()
  max_len = max(len(no_imp), len(imp))
  imp_news_yn = pd.DataFrame({
      'no_imp': pd.Series(no_imp + [np.nan] * (max_len - len(no_imp))),
      'imp': pd.Series(imp + [np.nan] * (max_len - len(imp)))
  })
  print('--- ì¤‘ìš”ê¸°ì‚¬ {}ê±´, ë¹„ì¤‘ìš”ê¸°ì‚¬ {}ê±´'.format(len(imp), len(no_imp)))


####################################################
# [4] í•´ë‹¹ ë‰´ìŠ¤ì˜ ì „ì²´ ë³¸ë¬¸ í¬ë¡¤ë§
####################################################

# ì¶”ì¶œí•œ url ì£¼ì†Œë¡œ ë‰´ìŠ¤ ì „ì²´ ë³¸ë¬¸ì„ í¬ë¡¤ë§
def article_full(df):
  print('\në‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤.')

  start_time = time.time()
  article_full = []
  for news_link in tqdm(df['url']):
    try:
      response = requests.get(news_link)
      soup = BeautifulSoup(response.content, 'html.parser')

      # ë„¤ì´ë²„ ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸ì˜ íƒœê·¸ëŠ” 'div', class ì´ë¦„ì€ 'article_body'
      article_body = soup.find('div', class_='newsct_article _article_body').get_text()
      article_body = article_body.replace('\t', '').replace('\n', '')
      article_full.append(article_body)
    except Exception as e:
      article_full.append(str(e))
      print("Failed to parse news content:", str(e))

  df['article_full'] = article_full
  end_time = time.time()
  print("--- ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§ ì™„ë£Œ --- ì†Œìš”ëœ ì‹œê°„(ì´ˆ):", end_time - start_time)


####################################################
# [4] í•´ë‹¹ ë‰´ìŠ¤ì˜ ì „ì²´ ë³¸ë¬¸ì„ ìš”ì•½í•˜ê¸°
####################################################

def gpt_summary(df):
  global summary_prompt
  print('\nì „ì²´ ë³¸ë¬¸ì„ ìš”ì•½í•©ë‹ˆë‹¤.')

  summary_prompt = '''
ë„ˆëŠ” ë‰´ìŠ¤ê¸°ì‚¬ë¥¼ ì½ê³  ë³´í—˜ì‚¬ì˜ ë””ì§€í„¸ ë³´ê³ ì„œ ì œì‘í•˜ëŠ” ìš”ì•½ ì „ë¬¸ê°€ì•¼.
[ì œëª©]ì€ ğŸ’¡ë¡œ ì‹œì‘í•˜ê³ , [ì£¼ìš”ê¸°ì—…orê¸°ê´€ëª…, ì£¼ëœ ë‚´ìš©] í˜•ì‹ìœ¼ë¡œ ì ì–´ì¤˜.

[ë‚´ìš©]ì˜ ì¡°ê±´ì€ ë‹¤ìŒê³¼ ê°™ì•„.
1. -ë¡œ ì‹œì‘í•˜ê³ , "ëª…ì‚¬í˜•"ìœ¼ë¡œ ëë‚´ì¤˜.(~í•¨. ~ì„)
2. ë³´í—˜ì‚¬ì—ì„œ ì‹¤ì œ ì ìš©í• ë§Œí•œ ë‚´ìš© ìœ„ì£¼ë¡œ ìì„¸í•˜ê³  ëª…í™•í•˜ê²Œ ìš”ì•½í•´ì¤˜.
3. [ì œëª©]ì— ìˆëŠ” ë‹¨ì–´ë¥¼ [ë‚´ìš©]ì— ë°˜ë³µí•˜ì§€ëŠ” ë§ˆ.

ì˜ˆì‹œ:
ğŸ’¡ NHë†í˜‘ìƒëª…, 'ë³´ì¥ë¶„ì„ ìë™ì œê³µ ì„œë¹„ìŠ¤' íŠ¹í—ˆ ì¶œì›
- 'ë³´ì¥ë¶„ì„ ìë™ì œê³µ ì„œë¹„ìŠ¤'ì— ëŒ€í•œ íŠ¹í—ˆ ì¶œì›ì„ ì™„ë£Œ
- ê³ ê°ì—ê²Œ ë³´ì¥ë¶„ì„ ë³´ê³ ì„œë¥¼ ì£¼ê¸°ì ìœ¼ë¡œ ì•Œë¦¼í†¡ì„ í†µí•´ ì œê³µí•˜ëŠ” ë‚´ìš©ì„ í¬í•¨, ê¸°ì¡´ì—ëŠ” ëª¨ì§‘ì¸ì„ í†µí•´ ì œê³µë˜ë˜ ì •ë³´ë¥¼ ê³ ê°ì—ê²Œ í™•ëŒ€Â·ì œê³µí•˜ê³ ì ê°œë°œ
- ì´ë¥¼ í†µí•´ ê³ ê°ì€ ì‹ ê³„ì•½, ë§Œê¸°, í•´ì§€ ë“±ì— ë”°ë¥¸ ì˜ˆìƒë³´í—˜ê¸ˆ ë° ë³´ì¥í˜„í™© ë³€ë™ì— ëŒ€í•œ ì •ë³´ë¥¼ ì‹ ì†í•˜ê²Œ ì œê³µ ê°€ëŠ¥
- ë†í˜‘ìƒëª…ì€ íŠ¹í—ˆ ì¶œì› ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‹œìŠ¤í…œ ê°œë°œì„ ì§„í–‰í•˜ë©°, ë‚´ë…„ ì¤‘ì— í•´ë‹¹ ì„œë¹„ìŠ¤ë¥¼ ë¡ ì¹­í•  ê³„íš


ì•„ë˜ì— ìˆëŠ” ë‰´ìŠ¤ê¸°ì‚¬ë¥¼ [ì œëª©]ì€ 1ì¤„, [ë‚´ìš©]ì€ 4~6ì¤„ë¡œ ìš”ì•½í•´ì¤˜.

-------------------------------------------------------------------------------------
ë‰´ìŠ¤ê¸°ì‚¬:
'''

  client = OpenAI()
  start_time = time.time()
  gpt_summary = []
  for i in tqdm(range(len(df))):
      completion = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
          {"role": "system", "content": "ë‰´ìŠ¤ê¸°ì‚¬ë¥¼ ì½ê³  ê¸ˆìœµ ë””ì§€í„¸ ë³´ê³ ì„œ ì œì‘í•˜ëŠ” ìš”ì•½ ì „ë¬¸ê°€"},
          {"role": "user", "content": summary_prompt + df['article_full'][i]}
        ],
      temperature=0 # ìµœëŒ€í•œ ë‹µë³€ì„ ë‹¤ë¥´ê²Œ í•˜ì§€ ì•Šë„ë¡ í•¨
      )
      gpt_summary.append(completion.choices[0].message.content)
  df['gpt_summary'] = gpt_summary
  end_time = time.time()
  print("--- GPTë¡œ ë‰´ìŠ¤ìš”ì•½ ì™„ë£Œ --- ì†Œìš”ëœ ì‹œê°„(ì´ˆ):", end_time - start_time)



####################################################
# [5] ìš”ì•½ ê²°ê³¼ í›„ì²˜ë¦¬
####################################################

# ìµœì¢… ìš”ì•½ëœ ë‚´ìš©ì„ ì½ê³  ê¸°ì‚¬ ì¤‘ìš”ë„ íŒì •
def ranking_imp_news(df):
  global imp_ranking_prompt
  print('\nìš”ì•½ëœ ë‰´ìŠ¤ì˜ ì¤‘ìš”ë„ë¥¼ íŒë³„í•©ë‹ˆë‹¤.')

  imp_ranking_prompt='''
ë„ˆëŠ” [ìš”ì•½ëœ ë‰´ìŠ¤ê¸°ì‚¬]ë¥¼ ì½ê³  ë‰´ìŠ¤ê¸°ì‚¬ì˜ ì¤‘ìš”ë„ë¥¼ íŒë³„í•˜ëŠ” íŒë³„ê°€ì•¼.

"ë³´í—˜ì‚¬ì˜ ë””ì§€í„¸ ì „í™˜ ë° ì‹ ì‚¬ì—… ë„ì…"ì˜ ê´€ì ì—ì„œ ë‰´ìŠ¤ê¸°ì‚¬ì˜ ì¤‘ìš”ë„ë¥¼ ìˆ«ì 1~5 ì¤‘ í•˜ë‚˜ë¥¼ ì¶œë ¥í•´ì¤˜.
ë‰´ìŠ¤ê¸°ì‚¬ê°€ ì¤‘ìš”í• ìˆ˜ë¡ ìˆ«ìê°€ í¬ê³ , ì¤‘ìš”í•˜ì§€ ì•Šì„ìˆ˜ë¡ ìˆ«ìê°€ ì‘ì•„.

ì˜ˆì‹œ)
[ìš”ì•½ëœ ë‰´ìŠ¤ê¸°ì‚¬]:
ë³´í—˜ì‚¬ì˜ AIê¸°ìˆ ì„ í™œìš©í•œ ê³ ê°ì„œë¹„ìŠ¤ ì—…ê·¸ë ˆì´ë“œ
- Aìƒëª…ë³´í—˜ì‚¬ê°€ decision treeë¥¼ ì´ìš©í•´ ê³ ê°ë¶„ë¥˜ë¥¼ í•˜ì˜€ë‹¤
-> 5

ëª…ì‹¬í•´.
ë„ˆëŠ” 1, 2, 3, 4, 5 ì¤‘ì— í•˜ë‚˜ì˜ ìˆ«ìë¥¼ ì¶œë ¥í•´ì•¼í•´
-------------------------------------------------------------------------------------
[ìš”ì•½ëœ ë‰´ìŠ¤ê¸°ì‚¬]:

'''

  client = OpenAI()
  start_time = time.time()
  imp_rank = []
  for i in tqdm(range(len(df))):
      completion = client.chat.completions.create(
        model="gpt-3.5-turbo", # gpt-3.5-turbo
        messages=[
          {"role": "system", "content": "ë‰´ìŠ¤ê¸°ì‚¬ë¥¼ ì½ê³  ë‰´ìŠ¤ì˜ ì¤‘ìš”ë„ë¥¼ íŒë³„í•˜ëŠ” íŒë³„ê°€"},
          {"role": "user", "content": imp_ranking_prompt + df['gpt_summary'][i]}
        ],
      temperature=0, # ìµœëŒ€í•œ ë‹µë³€ì„ ë‹¤ë¥´ê²Œ í•˜ì§€ ì•Šë„ë¡ í•¨
      max_tokens=1
      )
      imp_rank.append(completion.choices[0].message.content)
  df['imp_rank'] = imp_rank
  end_time = time.time()
  print("--- ì¤‘ìš”ë‰´ìŠ¤ íŒë³„ ì™„ë£Œ --- ì†Œìš”ëœ ì‹œê°„(ì´ˆ):", end_time - start_time)

# ì¤‘ìš”ë‰´ìŠ¤ top8 ì´ë‚´ë¡œ ê±°ë¥´ê¸°
def del_over8_news(df, max_num=8):
  news_num = len(df)
  if news_num > max_num:
    df = df.sort_values(by='imp_rank').head(8).reset_index(drop=True)
    print(f'--- ìµœì¢… ê¸°ì‚¬ê°€ {max_num}ê±´ì´ ë„˜ì–´, ì¤‘ìš”ë„ê°€ ë‚®ì€ {news_num}ê±´ì˜ ë‰´ìŠ¤ë¥¼ ì œê±°í•˜ì˜€ìŠµë‹ˆë‹¤.')
  else:
    df = df
    print(f'--- ìµœì¢… ê¸°ì‚¬ê°€ {max_num}ê±´ ì´í•˜ì´ë¯€ë¡œ ë‰´ìŠ¤ë¥¼ ì œê±°í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.')


####################################################
# [6] ë©”ì¼ ì „ì†¡
####################################################

def generate_newsletter(df):
  print('\nìš”ì•½ëœ ë‰´ìŠ¤ê¸°ì‚¬ë¥¼ ë©”ì¼ë¡œ ì „ì†¡í•©ë‹ˆë‹¤.')

  # Jinja2 í™˜ê²½ ì„¤ì •
  env = Environment(loader=FileSystemLoader('.'))
  template = env.get_template('newsletter_template_v2.html')

  # í˜„ì¬ ë‚ ì§œ ê°€ì ¸ì˜¤ê¸°
  current_date = datetime.now().strftime('%Y.%m.%d')

  # ë‰´ìŠ¤ ë§¨ìœ„ ìš”ì•½ ë§Œë“¤ê¸° (news_summary)
  Summary_list = df['gpt_summary']
  html_text = "<p>"
  for summary in Summary_list:
      summary = summary.split('-')[0].strip().replace('ğŸ’¡ ', 'ğŸ‘‰ ')
      html_text += summary + "<br>"
  html_text += "</p>"
  news_summary = html_text

  # ë‰´ìŠ¤ ë³¸ë¬¸ ë§Œë“¤ê¸°
  df['date'] = pd.to_datetime(df['pubDate'], format='%a, %d %b %Y %H:%M:%S %z').dt.strftime('%Y.%m.%d') # ë‰´ìŠ¤ë ˆí„°ì— ë“¤ì–´ê°ˆ ë‚ ì§œ ë³€ìˆ˜ ì¶”ê°€
  Date_list = df['date']
  Link_list = df['url']

  news_content = ""
  for summary, link, date in zip(Summary_list, Link_list, Date_list):
      title = summary.split('-')[0].strip()
      formatted_summary = summary.split('-', 1)[1].replace('\n', '<br>')
      news_content += f'''
<p class="news-title">{title}</p>
<p class="news-summary">- {formatted_summary}</p>
<p class="news-date">({date}, <a href="{link}">ë‰´ìŠ¤ ë°”ë¡œê°€ê¸°)</a><br><br></p>


'''

  # í…œí”Œë¦¿ ë Œë”ë§
  output = template.render(current_date=current_date, news_summary=news_summary, news_content=news_content)

  # ê²°ê³¼ë¥¼ HTML íŒŒì¼ë¡œ ì €ì¥
  with open('newsletter.html', 'w', encoding='utf-8') as f:
      f.write(output)

  print("--- ë‰´ìŠ¤ë ˆí„°ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")


# ë©”ì¼ ìˆ˜ì‹ ì ë¦¬ìŠ¤íŠ¸ load
def read_recipients_from_file(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        recipients = [line.strip() for line in file if line.strip()]
    return recipients


# ë‰´ìŠ¤ ë³´ë‚´ê¸°
def send_newsletter(email_id, email_pw, recipients, subject):
    message = MIMEMultipart()
    message['Subject'] = subject
    message['From'] = f"{email_id}@naver.com"
    message['To'] = ",".join(recipients)

    with open('newsletter.html', 'r', encoding='utf-8') as file:
        html_content = file.read()

    mimetext = MIMEText(html_content, 'html')
    message.attach(mimetext)

    try:
        # í•´ë‹¹ ê³„ì •ì˜ ë©”ì¼ í™˜ê²½ì„¤ì •ì— ë“¤ì–´ê°€ IMAP/SMTP ì„¤ì • 'ì‚¬ìš©í•¨'ìœ¼ë¡œ ì €ì¥
        server = smtplib.SMTP('smtp.naver.com', 587)  # í•´ë‹¹ ì„œë²„ì˜ ì„œë²„ëª… ë° í¬íŠ¸ë¡œ ë³€ê²½
        server.ehlo()
        server.starttls()
        server.login(email_id, email_pw)
        server.sendmail(message['From'], recipients, message.as_string())
        server.quit()
        print("\nEmail sent successfully!")
    except Exception as e:
        print(f"\nFailed to send email: {e}")



####################################################
# [7] ìµœì¢…ê²°ê³¼ì €ì¥
####################################################

def save_result(df):
  print('\nìµœì¢… ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.')
  df['date'] = pd.to_datetime(df['pubDate'], format='%a, %d %b %Y %H:%M:%S %z').dt.strftime('%Y.%m.%d') # ê²°ê³¼ ë“¤ì–´ê°ˆ ë‚ ì§œ ë³€ìˆ˜ ì¶”ê°€
  save_result = df[['title', 'date', 'gpt_summary', 'oid', 'news_name', 'news_rank', 'imp_rank', 'article_full', 'url', 'originallink']]
  prompt_df = pd.DataFrame({
      'ì¤‘ìš”ê¸°ì‚¬ë¶„ë¥˜ í”„ë¡¬í”„íŠ¸' : [imp_news_prompt],
      'ë‰´ìŠ¤ìš”ì•½ í”„ë¡¬í”„íŠ¸' : [summary_prompt],
      'ë‰´ìŠ¤ìš”ì•½ ì¤‘ìš”ë„ ì„ ë³„' : [imp_ranking_prompt]
      }).T

  with pd.ExcelWriter("{}_ë‰´ìŠ¤ìš”ì•½ê²°ê³¼_{}.xlsx".format(query, datetime.now(pytz.utc).astimezone(pytz.timezone('Asia/Seoul')).strftime('%m%d_%H:%M'))) as writer:
      save_result.to_excel(writer, sheet_name='ë‰´ìŠ¤ìš”ì•½ê²°ê³¼', index=False)
      imp_news_yn.to_excel(writer, sheet_name='ì¤‘ìš”ë‰´ìŠ¤ê¸°ì‚¬ë¶„ë¥˜', index=False)
      prompt_df.to_excel(writer, sheet_name='í”„ë¡¬í”„íŠ¸', index=False)
  print('--- ìµœì¢…ê²°ê³¼ ì €ì¥ ì‹œê°: ', datetime.now(pytz.utc).astimezone(pytz.timezone('Asia/Seoul')).strftime('%m%d_%H:%M'))



####################################################
# ì‹¤í–‰
####################################################

if __name__ == "__main__":
  config = configparser.ConfigParser()
  config.read('config.ini')

  ## API Key ì„¤ì •
  # ë„¤ì´ë²„ API ì¸ì¦ ì •ë³´ / malife ì•„ì´ë”” í™œìš©
  client_id = config['DEFAULT']['client_id']
  client_secret = config['DEFAULT']['client_secret']
  # GPT API Key
  os.environ["OPENAI_API_KEY"] = config['DEFAULT']['openai_api_key']

  # Input ê°’ ì„¤ì •
  query = 'ë³´í—˜ +AI' # ê²€ìƒ‰í‚¤ì›Œë“œ (+ì˜ ì•ì€ ë„ê³  ë’¤ëŠ” ë¶™ì—¬ì•¼ AIë¥¼ í¬í•¨í•œ ê²€ìƒ‰ê²°ê³¼ ë„ì¶œ)
  n = 200 # n*10ê°œì˜ ê¸°ì‚¬ì¶”ì¶œ
  max_num = 8 # ìµœì¢… ë‰´ìŠ¤ìš”ì•½ ê°œìˆ˜ ìƒí•œì„ 

  ## [1] ë„¤ì´ë²„ë‰´ìŠ¤ í¬ë¡¤ë§
  df = load_news(query, n)
  df_ = get_oid(df) # ì–¸ë¡ ì‚¬ oid ì¶”ì¶œ ë° matching

  ## [2] ì „ì²˜ë¦¬
  df_filtered = news_preprocessing(df_) # ê¸°ë³¸ ì „ì²˜ë¦¬
  df_filtered = del_dup_news(df_filtered, col='title') # ë‰´ìŠ¤ ì œëª© ì¤‘ë³µì œê±°
  gpt_imp_news(df_filtered) # ì¤‘ìš” ë‰´ìŠ¤ë§Œ ë½‘ê¸°
  df_imp = df_filtered[df_filtered.imp_news_yn=='1'].reset_index(drop=True) # ì¤‘ìš”ë‰´ìŠ¤ê¸°ì‚¬ë§Œ ìµœì¢… í…Œì´ë¸”ì— í• ë‹¹

  ## [3] ì „ì²´ ë³¸ë¬¸ í¬ë¡¤ë§
  article_full(df_imp)

  ## [4] ì „ì²´ ë³¸ë¬¸ ìš”ì•½
  gpt_summary(df_imp)

  # [5] ìš”ì•½ ê²°ê³¼ í›„ì²˜ë¦¬
  df_fin = del_dup_news(df_imp, col='gpt_summary') # ë‰´ìŠ¤ìš”ì•½ë‚´ìš© êµ°ì§‘í™”
  ranking_imp_news(df_fin) # ìµœì¢… ìš”ì•½ëœ ë‚´ìš©ì„ ì½ê³  ê¸°ì‚¬ ì¤‘ìš”ë„ íŒì •
  del_over8_news(df_fin, max_num=max_num) # ì¤‘ìš”ë‰´ìŠ¤ top8 ì´ë‚´ë¡œ ê±°ë¥´ê¸°

  # [6] ìµœì¢…ê²°ê³¼ ë©”ì¼ë¡œ ì „ì†¡
  generate_newsletter(df_fin) # ë‰´ìŠ¤ë ˆí„° ë§Œë“¤ê¸°(html)
  email_id = os.getenv("EMAIL_ID", config['DEFAULT']['email_id']) # í™˜ê²½ ë³€ìˆ˜ ê°€ì ¸ì˜¤ê¸° (ê¸°ë³¸ê°’ ì„¤ì •)
  email_pw = os.getenv("EMAIL_PW", config['DEFAULT']['email_pw'])
  recipients = read_recipients_from_file('recipients_list.txt')
  subject = f"{datetime.today().strftime('%Yë…„ %mì›” %dì¼')}ì˜ ë‰´ìŠ¤ë ˆí„°"
  send_newsletter(email_id, email_pw, recipients, subject) # ë‰´ìŠ¤ë ˆí„° ë³´ë‚´ê¸°

  # [7] ìµœì¢…ê²°ê³¼ ì €ì¥
  save_result(df_fin)







